import torch
import torch.nn as nn
import numpy as np
from internnav.model.utils.misc import rank0_print
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
from diffusion_policy.model.diffusion.positional_embedding import SinusoidalPosEmb
from internnav.model.encoder.navdp_backbone import *
import random

class NavDP_Policy_DPT_CriticSum_DAT(nn.Module):
    def __init__(self,
                 image_size=224,
                 memory_size=2,
                 predict_size=32,
                 temporal_depth=16,
                 heads=8,
                 token_dim=384,
                 vlm_token_dim=3584,
                 channels=3,
                 dropout=0.1,
                 scratch=False,
                 finetune=False,
                 use_critic=False,
                 input_dtype="bf16",
                 navdp_pretrained=None,
                 navdp_version=0.0,
                 device='cuda:0'):
        super().__init__()
        self.image_size = image_size
        self.memory_size = memory_size
        self.predict_size = predict_size
        self.temporal_depth = temporal_depth
        self.attention_heads = heads
        self.input_channels = channels
        self.dropout = dropout
        
        self.use_critic = use_critic
        self.token_dim = token_dim
        self.vlm_token_dim = vlm_token_dim
        if input_dtype == "bf16":
            self.input_dtype = torch.bfloat16
        else:
            self.input_dtype = torch.float32
        
        self.rgbd_encoder = DAT_RGBD_Patch_Backbone(image_size, token_dim, memory_size=memory_size, finetune=finetune, version=navdp_version)
        self.point_encoder = nn.Linear(3, self.token_dim)
        self.decoder_layer = nn.TransformerDecoderLayer(d_model=token_dim,
                                                        nhead=heads,
                                                        dim_feedforward=4 * token_dim,
                                                        dropout=dropout,
                                                        activation='gelu',
                                                        batch_first=True,
                                                        norm_first=True)
        self.decoder = nn.TransformerDecoder(decoder_layer=self.decoder_layer,
                                             num_layers=self.temporal_depth)

        self.input_embed = nn.Linear(3, token_dim)
        self.cond_pos_embed = nn.Parameter(torch.zeros((1, memory_size * 16 + 2, token_dim), dtype=self.input_dtype))
        self.out_pos_embed = nn.Parameter(torch.zeros((1, predict_size, token_dim), dtype=self.input_dtype))
        self.drop = nn.Dropout(dropout)
        self.time_emb = SinusoidalPosEmb(token_dim)
        
        self.noise_scheduler = DDPMScheduler(
            num_train_timesteps=20,
            beta_schedule='squaredcos_cap_v2',
            clip_sample=True,
            prediction_type='epsilon'
        )
        
        self.layernorm = nn.LayerNorm(token_dim)
        self.action_head = nn.Linear(token_dim, 3)
        self.critic_head = nn.Linear(token_dim, 1)
        
        self.tgt_mask = (torch.triu(torch.ones(predict_size, predict_size)) == 1).transpose(0, 1)
        self.tgt_mask = self.tgt_mask.float().masked_fill(self.tgt_mask == 0, float('-inf')).masked_fill(self.tgt_mask == 1, float(0.0))
        self.tgt_mask = self.tgt_mask.to(dtype=self.input_dtype)
        
        self.cond_critic_mask = torch.zeros((predict_size, 2 + memory_size * 16))
        self.cond_critic_mask[:, 0:2] = float('-inf')
        self.cond_critic_mask = self.cond_critic_mask.to(dtype=self.input_dtype)
        
        self.vlm_embed_mlp = nn.Sequential(
            nn.Linear(vlm_token_dim, vlm_token_dim//4),
            nn.ReLU(),
            nn.Linear(vlm_token_dim//4, vlm_token_dim//8),
            nn.ReLU(),
            nn.Linear(vlm_token_dim//8, token_dim)
        )
        
        self.goal_compressor = TokenCompressor(token_dim, 8, 1)
        
        self.pg_embed_mlp = nn.Sequential(
            nn.Linear(2, token_dim//2),
            nn.ReLU(),
            nn.Linear(token_dim//2, token_dim)
        )
        self.pg_pred_mlp = nn.Sequential(
            nn.Linear(token_dim, token_dim//2),
            nn.ReLU(),
            nn.Linear(token_dim//2, token_dim//4),
            nn.ReLU(),
            nn.Linear(token_dim//4, 2)
        )

        self.model_name = "NavDP_Policy_DPT_CriticSum_DAT"
        self.navdp_pretrained = navdp_pretrained
    
    def load_model(self):
        rank0_print(f"Loading navdp model: {self.model_name}")
        rank0_print(f"Pretrained: {self.navdp_pretrained}")
        
        if self.navdp_pretrained is None:
            rank0_print("No pretrained weights provided, initializing randomly.")
            return

        try:
            pretrained_dict = torch.load(self.navdp_pretrained)
            
            if 'state_dict' in pretrained_dict:
                pretrained_dict = pretrained_dict['state_dict']
            
            model_dict = self.state_dict()
            
            matched_dict = {k: v for k, v in pretrained_dict.items() 
                        if k in model_dict and v.size() == model_dict[k].size()}
            
            unmatched_pretrained = [k for k in pretrained_dict if k not in matched_dict]
            unmatched_model = [k for k in model_dict if k not in pretrained_dict]
            
            model_dict.update(matched_dict)
            self.load_state_dict(model_dict)
            
            rank0_print(f"Successfully loaded pretrained weights from {self.navdp_pretrained}")
            rank0_print(f"Loaded {len(matched_dict)}/{len(model_dict)} layers")
            
            if unmatched_pretrained:
                rank0_print("\nParameters in pretrained but NOT loaded:")
                for k in unmatched_pretrained:
                    if k in model_dict:
                        reason = f"size mismatch (pretrained: {pretrained_dict[k].size()}, model: {model_dict[k].size()})"
                    else:
                        reason = "not in model"
                    rank0_print(f"  - {k} ({reason})")
            
            if unmatched_model:
                rank0_print("\nParameters in model but NOT in pretrained:")
                for k in unmatched_model:
                    rank0_print(f"  - {k}")

        except Exception as e:
            rank0_print(f"Error loading pretrained weights: {str(e)}")
            rank0_print("Continuing with random initialization.")
    
    def sample_noise(self, action):
        noise = torch.randn(action.shape, dtype=action.dtype).to(action.device)
        timesteps = torch.randint(0, self.noise_scheduler.config.num_train_timesteps, (action.shape[0],)).long().to(action.device)
        time_embeds = self.time_emb(timesteps).unsqueeze(1).to(dtype=self.input_dtype)
        noisy_action = self.noise_scheduler.add_noise(action, noise, timesteps)
        noisy_action_embed = self.input_embed(noisy_action)
        return noise, time_embeds, noisy_action_embed
    
    def predict_noise(self, last_actions, timestep, goal_embed, rgbd_embed=None):
        action_embeds = self.input_embed(last_actions)
        time_embeds = self.time_emb(timestep.to(last_actions.device)).unsqueeze(1).to(dtype=last_actions.dtype)
        
        if rgbd_embed is not None:
            cond_embedding = torch.cat([time_embeds, goal_embed, rgbd_embed], dim=1) + self.cond_pos_embed[:, :self.memory_size*16+2, :]
        else:
            cond_embedding = torch.cat([time_embeds, goal_embed], dim=1) + self.cond_pos_embed[:, :2, :]

        cond_embedding = cond_embedding.repeat(action_embeds.shape[0], 1, 1)
        input_embedding = action_embeds + self.out_pos_embed[:, :self.predict_size, :]
        
        output = self.decoder(tgt=input_embedding, memory=cond_embedding, tgt_mask=self.tgt_mask)
        output = self.layernorm(output)
        output = self.action_head(output)
        return output
    
    def predict_pointgoal_action_async(self, vlm_tokens, input_images=None, input_depths=None, vlm_mask=None, sample_num=32):
        """
        Predict action sequence for point goal navigation using diffusion-based approach.

        This method generates a sequence of actions to reach a target point using 
        vision-language model (VLM) embeddings and RGB-D sensory inputs, leveraging
        a diffusion model to denoise action predictions.

        Args:
            vlm_tokens (Tensor): Token embeddings from vision-language model, 
                shape (batch_size, token_numbers, 3584)
            input_images (Tensor, optional): Input RGB images including memory frames,
                shape (batch_size, memory_frames, 224, 224, 3). 
                Defaults to None.
            input_depths (Tensor, optional): Input depth maps,
                shape (batch_size, memory_frames, 224, 224, 1). 
                Defaults to None.
            vlm_mask (Tensor, optional): Mask for VLM tokens indicating valid positions,
                shape (batch_size, token_numbers). 
                Defaults to None.
            sample_num (int, optional): Number of action sequences to sample through diffusion.
                Defaults to 32.

        Returns:
            Tensor: Predicted action trajectories after diffusion denoising,
                shape (sample_num * batch_size, prediction_size, 3)
        """
        with torch.no_grad():
            bs = vlm_tokens.shape[0]
            device_ = vlm_tokens.device
            if bs != 1:
                vlm_tokens = vlm_tokens[0:1]
                vlm_mask = vlm_mask[0:1]
                bs = 1
            
            if vlm_mask is not None:
                vlm_mask_ = vlm_mask.bool()
                vlm_mask = ~vlm_mask_
            
            vlm_tokens = self.vlm_embed_mlp(vlm_tokens)
            vlm_embed = self.goal_compressor(vlm_tokens, vlm_mask)
            
            rgbd_embed = self.rgbd_encoder(input_images, input_depths)
            
            noisy_action = torch.randn((sample_num * bs, self.predict_size, 3), dtype=vlm_embed.dtype).to(vlm_embed.device)
            naction = noisy_action
            
            self.noise_scheduler.set_timesteps(self.noise_scheduler.config.num_train_timesteps)
            for k in self.noise_scheduler.timesteps[:]:
                noise_pred = self.predict_noise(naction, k.unsqueeze(0), vlm_embed, rgbd_embed)
                naction = self.noise_scheduler.step(model_output=noise_pred,timestep=k,sample=naction).prev_sample
            
            current_trajectory = naction
            return current_trajectory
        
    def predict_pointgoal_action(self, vlm_tokens, input_images=None, input_depths=None, vlm_mask=None, sample_num=32):
        """
        Args:
            vlm_tokens: bs*sel_num, token_nums, 3584
            input_images: bs*sel_num, memory+1, 224, 224, 3
            input_depths: bs*sel_num, 1, 224, 224, 3
            vlm_mask: bs*sel_num, token_nums
        """
        with torch.no_grad():
            bs = vlm_tokens.shape[0]
            if bs != 1:
                vlm_tokens = vlm_tokens[0:1]
                vlm_mask = vlm_mask[0:1]
                bs = 1
            
            if vlm_mask is not None:
                vlm_mask_ = vlm_mask.bool()
                # mask==True parts will be ignored by transformer, but now vlm valid parts have mask==True!
                vlm_mask = ~vlm_mask_
            
            vlm_tokens = self.vlm_embed_mlp(vlm_tokens)
            vlm_embed = torch.mean(vlm_tokens, dim=1).unsqueeze(1)
            
            noisy_action = torch.randn((sample_num * bs, self.predict_size, 3), dtype=vlm_embed.dtype).to(vlm_embed.device)
            naction = noisy_action
            
            self.noise_scheduler.set_timesteps(self.noise_scheduler.config.num_train_timesteps)
            for k in self.noise_scheduler.timesteps[:]:
                noise_pred = self.predict_noise(naction, k.unsqueeze(0), vlm_embed)
                naction = self.noise_scheduler.step(model_output=noise_pred, timestep=k, sample=naction).prev_sample
                

            current_trajectory = naction
            return current_trajectory
        
            
            